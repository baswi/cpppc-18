---
format: Markdown
title: Session 7 Summary
category: course notes, cpppc
...

[Code Examples for Session 07](/session-07/)


# Excursions on Demoscene, Art, Highschool Degrees and Piano Lessons

Rockstar system programming: showcasing the swaggiest of reasons to learn C++

[Demo Programming for Beginners](https://www.youtube.com/watch?v=0suToaCS7WU)

- [Arte Documentary on Farbrausch](https://www.youtube.com/watch?v=hqh1iXIlyJk)
- [Conspiracy - Universal Sequence](https://www.youtube.com/watch?v=1PmRoLFtPRo)
- [Conspiracy - Chaos Theory](https://www.youtube.com/watch?v=ZfuierUvx1A)
- [fr-041: debris (live)](https://www.youtube.com/watch?v=4BMH9K1UDK8)
- [fr-08: .the .product](https://www.youtube.com/watch?v=Y3n3c_8Nn2Y)
- [Still - Coronoid](https://www.youtube.com/watch?v=zMInb4MCrm0)
- [Team PokeMe - SHizZLE](https://www.youtube.com/watch?v=Sgj5npyC068) and the
  [Making of SHizZLE](https://www.youtube.com/watch?v=81278bx1Sz0)
- [Drool](https://www.youtube.com/watch?v=rnBFEQDwhnk)
- [EmPatch in Ziphead](https://www.youtube.com/watch?v=rJYVDuEOLLQ)
- [EmPatch](https://www.youtube.com/watch?v=NKjqfiVlfJ4)


# Polymorphism


## Static Polymorphism (CRTP)


## Dynamic Polymorphism (virtual)



# Perfect Forwarding

**References**

- [Thomas Becker: rvalue References](http://thbecker.net/articles/rvalue_references/section_01.html) \
  Highly recommended, excellent in-depth series on rvalues, move semantics and
  perfect forwarding
- [Scott Meyers: Universal References in C++11](https://isocpp.org/blog/2012/11/universal-references-in-c11-scott-meyers)
- [Perfect Forwarding and Universal References](https://eli.thegreenplace.net/2014/perfect-forwarding-and-universal-references-in-c/)


**Secondary References**

- [Beware of Perfect Forwarding Constructors](https://mpark.github.io/programming/2014/06/07/beware-of-perfect-forwarding-constructors/)
- [Scott Meyers: Adventures in Perfect Forwarding](http://aristeia.com/TalkNotes/Facebook2012_PerfectForwarding.pdf)
- [Faster C++ Move Construction and Perfect Forwarding](https://www.gdcvault.com/play/1015458/Faster-C-Move-Construction-and)


We used *rvalue references* to implement move semantics (move-construction and
move-assignments) in classes. Any function can be overloaded for rvalue
references:

~~~c++
void foo(X&  x); // lvalue reference overload
void foo(X&& x); // rvalue reference overload

X x;
X foobar() { return X { }; }

foo(x);        // argument is lvalue: calls foo(X&)
foo(foobar()); // argument is rvalue: calls foo(X&&)
~~~

Essentially, rvalue references allow to select a function overload at compile
time depending on whether it is used as *lvalue* or *rvalue* in an expression.


## The Problem

With overloads for rvalue references, we would need at least two variants
of a function taking a single argument to achieve "ideal" avoidance of temporary
copies.

With two arguments, this is already getting awkward:

~~~c++
void bar(T &  a, T &  b);
void bar(T && a, T &  b);
void bar(T &  a, T && b);
void bar(T && a, T && b);
~~~ 

You see where this is going.


## The Solution


First, we have to recognize that the following *Reference Collapsing Rules*
exist since C++11:

~~~c++
A&  &  -> A&
A&  && -> A&
A&& &  -> A&
A&& && -> A&&
~~~

Secondly, there is a template argument deduction rule for function templates
that take an argument by *rvalue reference* to a template argument:

~~~c++
template<typename T>
void foo(T &&);
~~~

The following rules apply:

- When foo is called on an lvalue of type A: \
    $\Rightarrow$  T resolves to A& \
    $\Rightarrow$ reference collapsed: A&
- When foo is called on an rvalue of type A: \
    $\Rightarrow$ T resolves to A \
    $\Rightarrow$ A&&

### Forwarding Reference

In [lab session 5](/session-05/session-05), we learned declarations of `rvalue` references (actually: `xvalue`):

~~~c++
void wrapper(Foo && foo) {
   func(foo);
}
~~~

To tell whether you need `std::move` or not, just remember that it is equivalent to `static_cast<Foo &&>(foo)`. In this example, `foo` already is an `rvalue` reference so there is no need to `std::move`. It would not hurt but just have no effect.

Now I need you to relax and remember that high blood pressure leads to serious disease, so try not to get upset.

In the following example, parameters `rval_ref` and `fwd_ref` do not have the same type category:

~~~c++
void wrapper_rval(Foo && rval_ref);

template <class T>
void wrapper_fwdr(T && fwd_ref);
~~~

In `wrapper_univ`, the parameter is a *forwarding reference*

> Sometimes called *universal reference* in dated references). \
> Please note that the term *universal reference* has been coined by Scott Meyers and is
> not official terminology.

The correct definition:

**Forwarding Reference**
: > rvalue reference in a type deducing context




# Idiom: Function Template Wrappers

~~~c++
std::pair<int, int> one;
std::pair<int, int> two;

one = std::make_pair(10, 20);
two = std::make_pair(10.5, 'A'); // implicit conversion from pair<double,char>
~~~

~~~c++
#include <utility>

template<typename T, typename U>
std::pair<T, U> make_pair_wrapper(T && t, U && u) {
  return std::make_pair(std::forward<T>(t),
                        std::forward<U>(u));
}

int  a = 0;
auto p = make_pair_wrapper(a, 3);
~~~


# Type Deduction: auto and decltype

Example from [Effective Modern C++](https://www.oreilly.com/library/view/effective-modern-c/9781491908419/ch01.html):

~~~c++
// C++11
template<typename Container, typename Index> // works, but
auto Access(Container& c, Index i)           // requires
  -> decltype(c[i])                          // refinement
{
  // ...
  return c[i];
}

// C++14
template<typename Container, typename Index> // C++14;
auto Access(Container& c, Index i)           // not quite
{                                            // correct
  // ...
  return c[i];  // return type deduced from c[i]
}
~~~

Using `decltype(auto)`: 

~~~c++
// Perfect forwarding C++11 version
template<typename Container, typename Index>
auto
authAndAccess(Container&& c, Index i)
  -> decltype(std::forward<Container>(c)[i])
{
  authenticateUser();
  return std::forward<Container>(c)[i];
}
~~~

~~~c++
// Perfect forwarding C++14 version
template<typename Container, typename Index>
decltype(auto)
authAndAccess(Container&& c, Index i)
{
  authenticateUser();
  return std::forward<Container>(c)[i];
}
~~~


## Guideline: "almost always auto" (aaa)

Using `auto` when you supposedly know the eventual type of a variable might
appear sloppy and lazy to you. But quite the opposite is the case:
using `auto` by convention is actually considered a good practice.

Let's discuss the following declarations:

~~~c++
Widget w_dc_1;
Widget w_dc_2 = Widget();

Widget w_il_1 = { 2 };
Widget w_il_2   { 2 };

auto   w_a_1  = Widget();
auto   w_a_2  = Widget(4);
~~~






# Multithreading and Parallelism

Creating threads is easy:

```c++
#include <iostream>
#include <thread>
using namespace std;

void func(int x) {
    cout << "Inside thread " << x << endl;
}

int main() {
    thread th(&func, 100);
    th.join();
    cout << "Outside thread" << endl;
    return 0;
}
```


## Tasks

An even higher level of abstraction avoids the concept of threads
altogether and talks in terms of
tasks instead. Consider the following example:

```c++
#include <iostream>
#include <future>
#include <chrono>
using namespace std;

int square(int x) {
    return x * x;
}

int main() {
    auto a = async(&square, 10);
    int v = a.get();

    cout << "The thread returned " << v << endl;
    return 0;
}
```

The async construct uses an object pair called a promise and a future.
The former has made a promise to eventually provide a value. The future
is linked to the
promise and can at any time try to retrieve the value by get(). If the
promise hasn't
been fulfilled yet, it will simply wait until the value is ready. The
async hides most
of this for us, except that it returns in this case a future<int>
object. Again, since
the compiler knows what this call to async returns, we can use auto to
declare the future.

Exercise:

Use async to solve the sum of squares problem. Iterate up to 20 and add
your future<int>
objects to a vector<future<int>>. Then, finally iterate all your futures
and retrieve the
value and add it to your accumulator. This should be only a few
modifications from the
code above. 

## this\_thread

Let's make sure this really runs in parallel. Using the code from the
last exercise,
now add a cout that prints x inside square. Run your program again.
Every time you run
it, it should be listing the values of x in order. This seems awfully
deterministic and
is not characteristic of running things in parallel. We did start them
in that order, so
maybe the threads aren't overtaking each other. We can check this by
adding a sleep
inside square, which we can pretend is the heavy computation of x * x:

```c++
this_thread::sleep_for(chrono::milliseconds(100));
```

Note that all these seemingly global objects are in the std namespace,
but since we issued
using namespace std, we made them visible globally. Okay, run this and
time the execution.
They are clearly taking turns and they are not running in parallel. Use
cout to print `this_thread::get_id()`.

Since the main execution is also considered a thread, try printing the
thread ID inside
main using this same function. What does this tell you?

The function async by default gives the program the option of running it
asynchronously
or deferred. The latter means square will be called first when we call
get(), and it
will be executed in the same thread. Ideally, the program should make an
intelligent
decision, optimized for performance, but for some reason GCC always
defers, so let's not
give it a choice about it.

Change the call to async as follows:

```c++
async(launch::async, &square, ...)
```

Run it again, timing the execution.


**Note**

We got a 20 time speed up only because our threads aren't actually heavy
on the CPU while sleeping, so it's not an ideal surrogate for imagining
that x * x actually takes 100 ms of CPU time.
In the real case, the speed up would be largely determined by how many
cores we have on our computer.

Ideally, you should avoid starting more computationally intensive
threads than your computer can truly run in parallel, since otherwise
your CPU cores will start switching its attention
between different threads. Each switch is called a context switch and
comes with an overhead that will hurt performance.

## Shared Access

Let us imagine that x * x is a very costly operation and we want to
calculate the sum of squares up to a certain number.
It would make sense to parallelize the calulation of each square across
threads.

We can do something like this:

```c++
#include <iostream>
#include <vector>
#include <thread>
using namespace std;

int accum = 0;

void square(int x) {
    accum += x * x;
}

int main() {
    vector<thread> ths;
    for (int i = 1; i <= 20; i++) {
        ths.push_back(thread(&square, i));
    }

    for (auto& th : ths) {
        th.join();
    }
    cout << "accum = " << accum << endl;
    return 0;
}
```

This should sum all squares up to and including 20. We iterate up to
20, and launch a new thread in each iteration that we give the
assignment to. After this, we call join() on all our threads, which
is a blocking operation that waits for the thread to finish, before
continuing the execution.

This is important to do before we print accum, since otherwise our
threads might not be done yet. You should always join your threads
before leaving main, if you haven't already.

Before moving on, also note that C++11 offers more terse iteration
syntax of the vector class, very close in syntax to Java.
We are also using the keyword auto instead of specifying the data
type thread, which we can do whenever the compiler can unambiguously
guess what the correct type should be. We added an & to retrieve a
reference and not a copy of the object, since join changes the nature
of the object.

Now, run this. Chances are it spits out 2870, which is the correct
answer.

Let's list all distinct outputs from 1000 separate runs, including the count
for each output:

$$
\text{for} \, (i \in \{1 \dots 1000\} ; \quad \text{do} \quad \text{./a.out}; \quad \text{done} \, | \, sort \, | \, uniq -c
$$

We should see several runs with incorrect results.

When the compiler processes `accum += x * x`, this expression is non-atomic and
consists of machine-level instructions equivalent to this:

~~~c++
int temp = x * x;
temp    += accum;
accum    = temp;
~~~

With two threads interleaving, it could look like this:

```c++
// Thread 1             // Thread 2
int temp1 = accum;      int temp2 = accum;          // temp1 = temp2 = 0
                        temp2 += 2 * 2;             // temp2 = 4
temp1 += 1 * 1;                                     // temp1 = 1
                        accum = temp1;              // accum = 1
accum = temp2;                                      // accum = 4
```

We end up with accum as 4, instead of the correct 5.

Exercise:

Before we fix the race condition, since keeping accum as a global
variable is poor style, we would rather pass it into the thread.
Add a parameter `int& accum` to `square`. It is important that it's a
reference, since we want to be able to change the accumulator.
However, we can't simply call `thread(&square, accum, i)`, since it
will make a copy of accum and then call `square` with that copy.
To fix this, we wrap `accum` in `ref()`, making it
`thread(&square, ref(accum), i)`.


### Mutex

A mutex (mutual exlusion) allows us to encapsulate blocks of code that
should only be executed in one thread at a time. Keeping the main
function the same:

```c++
int accum = 0;
std::mutex accum_mutex;

void square(int x) {
    int temp = x * x;
    accum_mutex.lock();
    accum += temp;
    accum_mutex.unlock();
}
```

The problem should now be fixed.
The first thread that calls `lock()` gets the lock. During this
time, all other threads that call `lock()`, will simply halt, waiting at
that line for the mutex to be unlocked. It is important to introduce the
variable temp, since we want the `x * x` calculations to be outside the
lock-unlock block, otherwise we would be hogging the lock while we're
running our calculations.

As a final polish, you should use `std::lock_guard`, a mutex wrapper that
provides a convenient RAII-style mechanism for owning a mutex for the
duration of a scoped block.

In the following variant, you also have unambiguous control over the
locked section and obvious prevention of reordering:


```c++
int accum = 0;
std::mutex accum_mutex;

void square(int x) {
    int temp = x * x;
    {
      std::lock_guard<std::mutex> lock(accum_mutex);
      accum += temp;
    }
}
```


### Atomic

Recommended Talk:
[C++ atomics, from basic to advanced. What do they really do?](https://www.youtube.com/watch?v=ZQFzMfHIxng)

C++11 offers even nicer abstractions to solve this problem. For
instance, the atomic
container:

```c++
#include <atomic>

std::atomic<int> accum(0);

void square(int x) {
    accum += x * x;
}
```

We don't need to introduce temp here, since `x * x` will be evaluated
before handed off to accum, so it will be outside the atomic event.

### Condition Variables

It is a common scenario to have one thread wait for another thread
to finish processing, essentially sending a signal between the threads.

This can be done with mutexes, but it would be awkward.
It can also be done using a global boolean "ready flag" that is set by
one thread and polled by the other.

Since setting notified to true is atomic, this would not need a
`std::atomic` or a mutex. However, reacting on a flag with low latency
requires a busy loop, pumping CPU load to 100% in the thread.
We could add a sleep inside the polling loop to reduce CPU load, but
this effectively sets latency to the sleep period length.

A more principled way however is to add a call to wait for a
condition variable inside the for loop.

Condition variables can be spuriously awaken, so we still need a `ready`
flag. 


```c++
#include <iostream>
#include <thread>
#include <condition_variable>
#include <mutex>
#include <chrono>
#include <queue>

std::condition_variable cond_var;
std::mutex m;

int main() {
    int value  = 100;
    bool ready = false;

    std::thread consumer([&]() {
        std::unique_lock<std::mutex> lock(m);
        while (!ready) {
            cond_var.wait(lock);
        }
        cout << "The value is " << value << endl;
    });

    std::thread producer([&]() {
        value = 20;
        ready = true;
        cond_var.notify_one();
    });

    reporter.join();
    assigner.join();
    return 0;
}
```


## Shared Ownership


